客户端和服务器端之间的通信
服务器端：
1）使用socket函数构建一个sockfd
2）调用bind函数将该sockfd与具体的协议地址相连，即具体的IP地址和port
3）使用listen函数进行监听
4）如果收到客户端的连接请求，使用accept函数接收客户端的连接请求，并产生新的sockfd
5）开始与客户端发送和接收消息
6）关闭与客户端连接的sockfd
7）关闭监听sockfd

客户端：
1）使用socket函数构建一个sockfd
2）使用connect函数尝试与服务器端相连接
3）开始用服务器端发送和接收消息
4）关闭该sockfd

关于一些常见socket函数的介绍
https://juejin.cn/post/6931543528971436046

socket函数：int listenfd = socket(AF_INET, SOCK_STREAM, 0); 
bind函数：
    struct sockaddr_in serveraddr;
    serveraddr.sin_family = AF_INET;
    serveraddr.sin_addr.s_addr = inet_addr(SERVER_ADDRESS);
    serveraddr.sin_port = htons(SERVER_PORT);
    if (connect(clientfd, (struct sockaddr *)&serveraddr, sizeof(serveraddr)) == -1)

将文件描述符fd与具体的协议地址绑定上，即具体的IP地址 + port
INADDR_ANY：本机所有IP地址，相当于0.0.0.0 本机回环地址：127.0.0.1
服务器端的监听端口号一般是固定的，客户端的进程端口号一般是随机分配的，port是short型变量，0-65535 1024以下的是保留端口，用户程序不使用
1）当客户端使用bind将fd与o号端口绑定 没有影响
2）固定客户端端口号 只能调用一次connect 下一次调用时由于该端口已经使用了，不能connect

I/O复用中：select、poll、epoll的区别

先理解一些概念
用户空间和内核空间：操作系统的核心是内核，可以访问受到保存的内存空间，也有很多用户进程没有的权限。为了保护内核，把虚拟空间4G分为内核空间和用户空间，最高的1G空间是内核空间，其余是用户空间。

缓存I/O：又叫标准I/O。将I/O的数据存在文件系统的页缓存中，数据会先被拷贝到操作系统内核的缓冲区中，再从缓冲区中拷贝到应用程序的内存空间。TCP中的send和recv数据其实也是这样。发送方A有一个
发送缓冲区和内核缓冲区，接收方B也有一个接收缓冲区和内核缓冲区。recv函数的本质不是从网络上收取信息，而是从内核缓冲区把数据拷贝到应用程序的接收缓冲区。拷贝后的数据会从内核缓冲区中移除

正常的一次I/O访问，比如read操作，数据会先被拷贝到操作系统的内核缓冲区，然后再拷贝到应用程序地址空间。
因为这种阶段，所以有五种I/O模式，阻塞I/O，非阻塞I/O，I/O多路复用，信号驱动I/O，异步I/O
阻塞I/O：应用进程调用recvfrom函数，会一直阻塞到数据准备好，并从内核缓冲区拷贝到应用进程缓冲区为止
非阻塞I/O：应用程序不断调用recvfrom函数，没有准备好数据时不阻塞，而是返回一个错误，不断轮询，如果可以了再进行操作。
I/O复用：一个进程同时等待多个文件描述符，其中任意一个进入读就绪状态，函数就返回，然后相应的应用进程就调用recvfrom系统调用将数据从内核缓冲区拷贝如进程缓冲区。

先理解阻塞和非阻塞的概念：conncet()、accept()这些函数就是阻塞函数，线程执行到这里没有某个事件，程序就会阻塞在这里。非阻塞函数就是select这种，轮循函数，循环询问文件节点，设置超时时间，超过该时间，
就跳过代码继续往下执行。非阻塞：一旦执行必定返回，以返回值的不同来反映函数执行情况，若事件没有发生就返回值进行提醒，线程继续往下执行，这样工作效率高，也能知道具体的文件描述符的变化情况。

select用来检测一组socket中是否有事件就绪。读事件 写事件 异常事件
int select(int maxfds, fd_set* readfds, fd_set* writefds, fd_set* errorfds, struct timeval* timeout)
maxfdp：描述需要检测的文件描述符（fd）的最大值+1。
fd_set：是一个结构体，一共有1024bit，这是select函数支持的最大fd数量。每次添加一个fd，就是用的位图法，把这一位置为1。
timeout：超时时间，可select处于三个状态
1）以NULL形式传入，select置为阻塞状态，一直等到监视文件描述符中某个文件描述符发生变化为止，因为时间是无限
2）0 纯粹的非阻塞函数，立刻就返回0超时
3）>0就是正常的函数

返回负值错误，0等待超时，正值某些文件可读可写或出错
在是呀select函数时，其实就是将需要检测的fd全部遍历一遍，看是否有准备好的，在timeout中如果没有就会一直休眠，不是阻塞。
select可以跨平台，但是单一线程能监视的文件描述符有限制，一般为1024。
注意，select函数轮询fd时，是在内核空间进行这样的操作，所以每次轮询要将fd从用户空间拷贝到内核空间。

select函数的过程：
1）用户线程调用select，将fd从用户空间拷贝到内核空间
2）内核在内核空间对fd遍历检查，如果没有就进入休眠，但是不阻塞，如果有就绪的fd或者时间到，就会返回函数
3）用户拿到就绪fd数量后，对fd进行遍历，找出就绪的fd
4）开始读写操作

所有select有很多缺点
1）有最大连接数，一般是1024，因为fd就是用位图法来查看有没有fd变化，最多1024位
2）每次设置的时候需要设置maxfd
3）检测fd是否变化时，要将fd从用户空间拷贝到内核空间
4）就算检测到了变化，用户空间也要遍历一遍fd，才知道是哪个fd就绪了

poll函数：int poll (struct pollfd *fds, unsigned int nfds, int timeout);
struct pollfd {
    int fd; /* 待检测的事件的fd */
    short events; /* 关心的事件组合 */
    short revents; /* 检测后得到的事件类型 */
};
nfds是待检测的fd的数量，events是用户设置的事件的种类，revents是poll函数返回时内核设置的，表示该fd发生了什么
基本流程与select一样，只不过
1）用户线程调用poll系统调用，将文件描述符链表拷贝到内核空间
2）内核对fd遍历一遍，如果没有就绪的就休眠，直达有就绪的fd到达或者timeout到
3）返回用户线程就绪的fd数量
4）用户线程遍历一次文件fd，找出就绪的文件描述符以及具体发生了什么事件
5）用户线程进行操作

有如下优点：
1）不用设置maxfd
2）没有最大连接的限制，因为是链式存储结构，没有数组的长度限制

epoll：
// 数据结构
// 每一个epoll对象都有一个独立的eventpoll结构体
// 红黑树用于存放通过epoll_ctl方法向epoll对象中添加进来的事件
// epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可
struct eventpoll {
    /*红黑树的根节点，这颗树存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表存储所有就绪的文件描述符*/
    struct list_head rdlist;
};
// API
int epoll_create(int size); // 内核中间加一个 eventpoll 对象，把所有需要监听的 socket 都放到 eventpoll 对象中
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);// epoll_wait 检测双链表中是否有就绪的文件描述符，如果有，则返回

epoll_create创建一个eventpoll对象，每一个epoll对象都有一个独立的eventpoll结构体，用红黑树存储需要检测的事件，用双链表存储就绪的fd
epoll_ctl添加，修改，删除fd到创建好的eventpoll结构体
epoll_wait调用，检测链表中是否有数据，有的话返回具体的就绪文件符的数量

缺点：只能在Linux上工作
优点：
1）不需要频繁的从用户空间到内存空间拷贝fd集合，使用了内存映射mmap技术
2）有就绪事件时，系统注册的回调函数就会被调用，把就绪的fd放入就绪链表中
3）时间复杂度是常数，当有事件就绪时，epoll_wait只需要检测就绪链表中是否有数据，有就返回。

select、poll、epoll的区别
底层数据结构：数组存储文件描述符   链表存储文件描述符    红黑树存储监控的文件描述符，双向链表存储就绪的文件描述符
如何获取就绪的fd 遍历fd           遍历链表             回调
时间复杂度       n               n                    1
FD数据拷贝       select和poll每次都要从用户内存拷贝到内核空间   epoll使用内存映射
最大连接数       有限制          无限制                无限制

epool的两种触发模式：
LT（水平触发）：如果感兴趣的事件还没有被处理完，比如读事件没有读完数据，还会被继续触发
ET（边缘触发）：如果感兴趣的事件被触发后，就会返回，但是就不会再继续触发了。所以ET模式必须保证socket是非阻塞模式，这样的话就算没有数据可读，fd不会一直阻塞；每次调用read和write时要保证把所有数据
处理结束，因为ET只会触发一次。

线程池的原理和实现
https://wangpengcheng.github.io/2019/05/17/cplusplus_theadpool/
原理：线程池就是一组线程。我们需要异步执行一些任务，这些任务存在于整个任务周期，与其让操作系统频繁为我们创建和销毁线程，不如创建一组在程序生命周期内不会退出的线程。
基本要求：当有任务需要执行时，线程可以自动拿到任务并执行，在没有任务到来时线程处于阻塞或者睡眠状态。
线程池的组成：任务队列 线程池 完成队列

实现步骤：
1）构造任务类和线程池类，线程池类中主要定义互斥量，用于互斥访问任务队列，条件变量，用于唤醒线程，线程数组，用于存储线程的sp，bool变量判断该线程池是否停止工作
2）初始化线程池中的线程数量 先构建一部分线程
3）处理函数中先将线程设置为待唤醒状态
4）向任务队列中条件任务，每次唤醒一个待唤醒的线程，进行处理
ps：这里要注意虚假唤醒的情况 while (m_taskList.empty()) {m_cv.wait(guard);} 要在while循环中等待信号，这样notify_one一个线程，可能多个竞争该唤醒条件的线程都会被唤醒，但是只有一个线程可以通过
while循环，开始处理
5）停止线程池工作 将bool变量设置为0 唤醒所有阻塞线程 并释放掉这些线程
6）remove掉所有的task

线程池中的线程数量是依据什么确定的？
线程池中的线程数量最直接的限制因素是CPU的处理器的数量。如果CPU是4核的，对于CPU密集型的任务（比如剪辑视频等消耗CPU计算资源的任务），线程池中线程数量最好也设置为4（或者+1防止其他因素造成的线程阻塞）；
对于IO密集型的任务，一般要多于CPU的核数，因为线程间竞争的不是CPU的资源而是IO。在一些线程I/O阻塞时，另一些线程是可以处理的，如果这时线程数量不够，就浪费时间了，造成CPU空闲。
公式：最佳线程数 = CPU当前可使用的cores数 * 当前CPU的利用率 * （1 + CPU等待时间 / CPU处理时间）

Reactor模式和Proactor模式：
I/O操作主要分为两步，第一步是内核缓冲区接受数据，第二步是将内核缓冲区的数据拷贝给进程空间。
Reactor模式是同步I/O模式。主线程只负责监听文件描述符上是否有事件发生，若有，就交给工作线程处理，工作线程依然要调用系统调用。
优点：实现简单，对于耗时短的处理场景高效；事件都是串行执行的，不需要加锁处理；
缺点：处理耗时长的操作会造成事件分发的阻塞，影响到后续事件的处理

Proactor模式是异步I/O模式。所有的I/O操作都交给主线程和内核，工作线程只负责处理逻辑。
优点：性能更高，能够处理耗时长的并发场景
缺点：实现逻辑复杂；依赖操作系统对异步的支持，目前实现了纯异步操作的操作系统少

适用场景
Reactor：同时接收多个服务请求，并且依次同步的处理它们的事件驱动程序； 
Proactor：异步接收和同时处理多个服务请求的事件驱动程序；







